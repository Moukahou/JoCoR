{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CNN, loss_coteaching, JoCor_loss\n",
    "from data.cifar import CIFAR10\n",
    "from config import opt\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual noise rate is 0.20076\n"
     ]
    }
   ],
   "source": [
    "data = CIFAR10()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=data,batch_size=128,drop_last=True,shuffle=True)\n",
    "\n",
    "test_data = CIFAR10(train=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,batch_size=128)\n",
    "\n",
    "cnn1 = CNN().cuda()\n",
    "optimizer1 = torch.optim.Adam(cnn1.parameters(), lr= opt.learning_rate)\n",
    "\n",
    "cnn2 = CNN().cuda()\n",
    "optimizer2 = torch.optim.Adam(cnn2.parameters(), lr= opt.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_schedule(opt):\n",
    "\n",
    "    alpha_plan = [opt.learning_rate] * opt.n_epoch\n",
    "    beta_plan = [opt.mom1] * opt.n_epoch\n",
    "    for i in range(opt.epoch_decay_start, opt.n_epoch):\n",
    "        alpha_plan[i] = float(opt.n_epoch - i) / (opt.n_epoch - opt.epoch_decay_start) * opt.learning_rate\n",
    "        beta_plan[i] = opt.mom2\n",
    "    rate_schedule = np.ones(opt.n_epoch)* opt.forget_rate\n",
    "    rate_schedule[:opt.num_gradual] = np.linspace(0, opt.forget_rate**opt.exponent, opt.num_gradual)\n",
    "    return alpha_plan, beta_plan, rate_schedule\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, alpha_plan, beta_plan):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr']=alpha_plan[epoch]\n",
    "        param_group['betas']=(beta_plan[epoch], 0.999)\n",
    "\n",
    "def train(train_loader,epoch, cnn1, cnn2, optimizer1, optimizer2, rate_schedule):\n",
    "    correct_1 = 0\n",
    "    correct_2 = 0\n",
    "    \n",
    "    loss_t_total = 0\n",
    "    total_instance = 0 \n",
    "    \n",
    "    loss_instance = 0\n",
    "    for i,(imgs, labels, index) in enumerate(train_loader):\n",
    "        y1 = cnn1(imgs.cuda())\n",
    "        y2 = cnn2(imgs.cuda())\n",
    "\n",
    "        target = labels.long().cuda()\n",
    "        loss_t, num_remember = JoCor_loss(y1, y2, target, rate_schedule[epoch], 0.85)\n",
    "\n",
    "        correct_1 += sum(y1.argmax(axis = 1) ==  target)\n",
    "        correct_2 += sum(y2.argmax(axis = 1) ==  target)\n",
    "        \n",
    "        total_instance += imgs.shape[0]\n",
    "        loss_instance += num_remember\n",
    "        loss_t_total += loss_t.item()\n",
    "\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer1.zero_grad()\n",
    "        \n",
    "        loss_t.backward()\n",
    "        \n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "    \n",
    "    return loss_t_total / loss_instance , correct_1.long().item() / total_instance , correct_2.long().item() / total_instance\n",
    "\n",
    "def test(test_loader, cnn1, cnn2):\n",
    "    correct_1 = 0\n",
    "    correct_2 = 0\n",
    "    \n",
    "    total = 0\n",
    "    for i,(imgs, labels, index) in enumerate(test_loader):\n",
    "        y1 = cnn1(imgs.cuda())\n",
    "        y2 = cnn2(imgs.cuda())\n",
    "        target = labels.long().cuda()\n",
    "        total += imgs.shape[0]\n",
    "        correct_1 += sum(y1.argmax(axis = 1) ==  target)\n",
    "        correct_2 += sum(y2.argmax(axis = 1) ==  target)\n",
    "        \n",
    "        \n",
    "    return correct_1.long().item()/ total, correct_2.long().item()/ total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 |loss:0.0049 |acc1:0.272 | acc2:0.274 |acc1_t:0.415 | acc2_t:0.416\n",
      "epoch 1 |loss:0.0044 |acc1:0.382 | acc2:0.383 |acc1_t:0.502 | acc2_t:0.502\n",
      "epoch 2 |loss:0.0042 |acc1:0.439 | acc2:0.439 |acc1_t:0.555 | acc2_t:0.554\n",
      "epoch 3 |loss:0.0039 |acc1:0.478 | acc2:0.478 |acc1_t:0.608 | acc2_t:0.608\n",
      "epoch 4 |loss:0.0037 |acc1:0.509 | acc2:0.509 |acc1_t:0.630 | acc2_t:0.638\n",
      "epoch 5 |loss:0.0035 |acc1:0.529 | acc2:0.528 |acc1_t:0.654 | acc2_t:0.657\n",
      "epoch 6 |loss:0.0034 |acc1:0.547 | acc2:0.549 |acc1_t:0.672 | acc2_t:0.669\n",
      "epoch 7 |loss:0.0032 |acc1:0.560 | acc2:0.560 |acc1_t:0.685 | acc2_t:0.682\n",
      "epoch 8 |loss:0.0030 |acc1:0.572 | acc2:0.572 |acc1_t:0.688 | acc2_t:0.699\n",
      "epoch 9 |loss:0.0029 |acc1:0.581 | acc2:0.580 |acc1_t:0.701 | acc2_t:0.703\n",
      "epoch 10 |loss:0.0028 |acc1:0.590 | acc2:0.590 |acc1_t:0.713 | acc2_t:0.709\n",
      "epoch 11 |loss:0.0027 |acc1:0.599 | acc2:0.600 |acc1_t:0.722 | acc2_t:0.723\n",
      "epoch 12 |loss:0.0026 |acc1:0.607 | acc2:0.608 |acc1_t:0.729 | acc2_t:0.728\n",
      "epoch 13 |loss:0.0025 |acc1:0.615 | acc2:0.616 |acc1_t:0.739 | acc2_t:0.738\n",
      "epoch 14 |loss:0.0024 |acc1:0.622 | acc2:0.622 |acc1_t:0.743 | acc2_t:0.743\n",
      "epoch 15 |loss:0.0024 |acc1:0.629 | acc2:0.627 |acc1_t:0.753 | acc2_t:0.748\n",
      "epoch 16 |loss:0.0023 |acc1:0.635 | acc2:0.635 |acc1_t:0.760 | acc2_t:0.758\n",
      "epoch 17 |loss:0.0023 |acc1:0.640 | acc2:0.641 |acc1_t:0.762 | acc2_t:0.764\n",
      "epoch 18 |loss:0.0022 |acc1:0.645 | acc2:0.644 |acc1_t:0.768 | acc2_t:0.766\n",
      "epoch 19 |loss:0.0022 |acc1:0.650 | acc2:0.649 |acc1_t:0.778 | acc2_t:0.772\n",
      "epoch 20 |loss:0.0022 |acc1:0.656 | acc2:0.654 |acc1_t:0.774 | acc2_t:0.779\n",
      "epoch 21 |loss:0.0021 |acc1:0.659 | acc2:0.660 |acc1_t:0.782 | acc2_t:0.782\n",
      "epoch 22 |loss:0.0021 |acc1:0.663 | acc2:0.663 |acc1_t:0.786 | acc2_t:0.785\n",
      "epoch 23 |loss:0.0020 |acc1:0.668 | acc2:0.667 |acc1_t:0.786 | acc2_t:0.790\n",
      "epoch 24 |loss:0.0020 |acc1:0.672 | acc2:0.671 |acc1_t:0.794 | acc2_t:0.793\n",
      "epoch 25 |loss:0.0020 |acc1:0.675 | acc2:0.674 |acc1_t:0.795 | acc2_t:0.796\n",
      "epoch 26 |loss:0.0019 |acc1:0.678 | acc2:0.677 |acc1_t:0.798 | acc2_t:0.799\n",
      "epoch 27 |loss:0.0019 |acc1:0.680 | acc2:0.681 |acc1_t:0.800 | acc2_t:0.804\n",
      "epoch 28 |loss:0.0019 |acc1:0.683 | acc2:0.683 |acc1_t:0.806 | acc2_t:0.807\n",
      "epoch 29 |loss:0.0018 |acc1:0.687 | acc2:0.687 |acc1_t:0.806 | acc2_t:0.805\n",
      "epoch 30 |loss:0.0018 |acc1:0.690 | acc2:0.689 |acc1_t:0.808 | acc2_t:0.804\n",
      "epoch 31 |loss:0.0018 |acc1:0.693 | acc2:0.693 |acc1_t:0.808 | acc2_t:0.807\n",
      "epoch 32 |loss:0.0018 |acc1:0.695 | acc2:0.695 |acc1_t:0.811 | acc2_t:0.815\n",
      "epoch 33 |loss:0.0017 |acc1:0.698 | acc2:0.697 |acc1_t:0.816 | acc2_t:0.812\n",
      "epoch 34 |loss:0.0017 |acc1:0.701 | acc2:0.701 |acc1_t:0.817 | acc2_t:0.815\n",
      "epoch 35 |loss:0.0017 |acc1:0.702 | acc2:0.703 |acc1_t:0.817 | acc2_t:0.820\n",
      "epoch 36 |loss:0.0017 |acc1:0.707 | acc2:0.706 |acc1_t:0.822 | acc2_t:0.821\n",
      "epoch 37 |loss:0.0017 |acc1:0.707 | acc2:0.707 |acc1_t:0.820 | acc2_t:0.824\n",
      "epoch 38 |loss:0.0016 |acc1:0.710 | acc2:0.710 |acc1_t:0.821 | acc2_t:0.826\n",
      "epoch 39 |loss:0.0016 |acc1:0.712 | acc2:0.712 |acc1_t:0.820 | acc2_t:0.823\n",
      "epoch 40 |loss:0.0016 |acc1:0.714 | acc2:0.714 |acc1_t:0.827 | acc2_t:0.829\n",
      "epoch 41 |loss:0.0016 |acc1:0.715 | acc2:0.715 |acc1_t:0.829 | acc2_t:0.829\n",
      "epoch 42 |loss:0.0015 |acc1:0.719 | acc2:0.719 |acc1_t:0.830 | acc2_t:0.829\n",
      "epoch 43 |loss:0.0015 |acc1:0.720 | acc2:0.721 |acc1_t:0.833 | acc2_t:0.831\n",
      "epoch 44 |loss:0.0015 |acc1:0.722 | acc2:0.722 |acc1_t:0.832 | acc2_t:0.834\n",
      "epoch 45 |loss:0.0015 |acc1:0.724 | acc2:0.723 |acc1_t:0.836 | acc2_t:0.835\n",
      "epoch 46 |loss:0.0015 |acc1:0.726 | acc2:0.726 |acc1_t:0.838 | acc2_t:0.832\n",
      "epoch 47 |loss:0.0015 |acc1:0.727 | acc2:0.727 |acc1_t:0.833 | acc2_t:0.833\n",
      "epoch 48 |loss:0.0014 |acc1:0.730 | acc2:0.729 |acc1_t:0.834 | acc2_t:0.837\n",
      "epoch 49 |loss:0.0014 |acc1:0.730 | acc2:0.730 |acc1_t:0.834 | acc2_t:0.835\n",
      "epoch 50 |loss:0.0014 |acc1:0.732 | acc2:0.733 |acc1_t:0.839 | acc2_t:0.837\n",
      "epoch 51 |loss:0.0014 |acc1:0.734 | acc2:0.734 |acc1_t:0.838 | acc2_t:0.841\n",
      "epoch 52 |loss:0.0014 |acc1:0.735 | acc2:0.736 |acc1_t:0.841 | acc2_t:0.842\n",
      "epoch 53 |loss:0.0014 |acc1:0.737 | acc2:0.738 |acc1_t:0.841 | acc2_t:0.843\n",
      "epoch 54 |loss:0.0014 |acc1:0.738 | acc2:0.737 |acc1_t:0.843 | acc2_t:0.843\n",
      "epoch 55 |loss:0.0013 |acc1:0.740 | acc2:0.739 |acc1_t:0.842 | acc2_t:0.844\n",
      "epoch 56 |loss:0.0013 |acc1:0.743 | acc2:0.741 |acc1_t:0.843 | acc2_t:0.845\n",
      "epoch 57 |loss:0.0013 |acc1:0.743 | acc2:0.744 |acc1_t:0.845 | acc2_t:0.844\n",
      "epoch 58 |loss:0.0013 |acc1:0.744 | acc2:0.743 |acc1_t:0.847 | acc2_t:0.848\n",
      "epoch 59 |loss:0.0013 |acc1:0.746 | acc2:0.746 |acc1_t:0.848 | acc2_t:0.848\n",
      "epoch 60 |loss:0.0013 |acc1:0.747 | acc2:0.746 |acc1_t:0.844 | acc2_t:0.848\n",
      "epoch 61 |loss:0.0012 |acc1:0.749 | acc2:0.748 |acc1_t:0.849 | acc2_t:0.848\n",
      "epoch 62 |loss:0.0012 |acc1:0.750 | acc2:0.751 |acc1_t:0.851 | acc2_t:0.849\n",
      "epoch 63 |loss:0.0012 |acc1:0.751 | acc2:0.751 |acc1_t:0.845 | acc2_t:0.851\n",
      "epoch 64 |loss:0.0012 |acc1:0.752 | acc2:0.753 |acc1_t:0.853 | acc2_t:0.851\n",
      "epoch 65 |loss:0.0012 |acc1:0.754 | acc2:0.753 |acc1_t:0.851 | acc2_t:0.851\n",
      "epoch 66 |loss:0.0012 |acc1:0.756 | acc2:0.755 |acc1_t:0.852 | acc2_t:0.850\n",
      "epoch 67 |loss:0.0012 |acc1:0.756 | acc2:0.756 |acc1_t:0.852 | acc2_t:0.853\n",
      "epoch 68 |loss:0.0012 |acc1:0.757 | acc2:0.757 |acc1_t:0.850 | acc2_t:0.853\n",
      "epoch 69 |loss:0.0011 |acc1:0.758 | acc2:0.758 |acc1_t:0.853 | acc2_t:0.855\n",
      "epoch 70 |loss:0.0011 |acc1:0.760 | acc2:0.759 |acc1_t:0.850 | acc2_t:0.849\n",
      "epoch 71 |loss:0.0011 |acc1:0.760 | acc2:0.759 |acc1_t:0.853 | acc2_t:0.854\n",
      "epoch 72 |loss:0.0011 |acc1:0.761 | acc2:0.761 |acc1_t:0.857 | acc2_t:0.849\n",
      "epoch 73 |loss:0.0011 |acc1:0.762 | acc2:0.762 |acc1_t:0.856 | acc2_t:0.856\n",
      "epoch 74 |loss:0.0011 |acc1:0.763 | acc2:0.764 |acc1_t:0.856 | acc2_t:0.855\n"
     ]
    }
   ],
   "source": [
    "alpha_plan, beta_plan, rate_schedule = set_schedule(opt) \n",
    "\n",
    "for epoch in range(opt.n_epoch):\n",
    "    cnn1.train()\n",
    "    adjust_learning_rate(optimizer1, epoch, alpha_plan, beta_plan)\n",
    "    cnn2.train()\n",
    "    adjust_learning_rate(optimizer2, epoch, alpha_plan, beta_plan)\n",
    "    \n",
    "    loss_1_total, correct_1, correct_2 = train(train_loader,epoch, cnn1, cnn2, optimizer1, optimizer2, rate_schedule)\n",
    "    \n",
    "    cnn1.eval()\n",
    "    cnn2.eval()\n",
    "    acc1_test, acc2_test = test(test_loader, cnn1, cnn2)\n",
    "    print('epoch',epoch,'|loss:' '%.4f' % loss_1_total ,'|acc1:''%.3f' % correct_1 , '| acc2:''%.3f' % correct_2, '|acc1_t:''%.3f' % acc1_test , '| acc2_t:''%.3f' % acc2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
